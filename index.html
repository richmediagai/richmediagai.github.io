<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="accv, workshop, accv-2024, accv2024, computer vision, machine learning, generative model, diffusion, LLM">

  <link rel="shortcut icon" href="./img/icon.png">


  <title>Rich Media with Generative AI</title>
  <meta name="description" content="Workshop in Conjunction with ACCV 2024 ---">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="Workshop on Rich Media with Generative AI"/>
  <meta property="og:url" content=""/>
  <meta property="og:description" content="Workshop in Conjunction with ACCV 2024"/>
  <meta property="og:site_name" content="Workshop on Rich Media with Generative AI"/>
  <meta property="og:image" content=""/>
  <meta property="og:image:url" content=""/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:creator" content="">
  <meta name="twitter:title" content=""/>
  <meta name="twitter:image" content="">
  <meta name="twitter:url" content=""/>           
  <meta name="twitter:description" content=""/>

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="./css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="./css/main.css" media="screen,projection">

  <!-- Font Awesome -->
  <script src="https://kit.fontawesome.com/ff6e9b10da.js" crossorigin="anonymous"></script>
</head>

  <body>


    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">

    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#syllabus">Syllabus</a></li>
		<li><a href="#papers">Paper Submissions</a></li>
    <li><a href="#challenges">Challenges</a></li>
		<li><a href="#speakers">Speakers</a></li>
        <li><a href="#talks">Schedule</a></li>
        <li><a href="#organizers">Orgnizers</a></li>
      </ul>
    </div>

  </div>
</div>

    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
  <center><h2>ACCV 2024 Workshop on</h2></center>
    <center><h1>Rich Media with Generative AI</h1></center>
        <center><b>Date: </b>Monday 8 Dec or Tuesday 9 Dec 9:00 am - 5:00 pm (Vienam Time/GMT+7), 2024</center>
        <center>(specific date to be decided by ACCV)</center>
        <center><b>Location: </b>Hanoi Vienam + Zoom</center>
        <!-- <center>To obtain the Zoom link, kindly fill out this <a href="https://forms.gle/4m8E8vkmTJ4D8vQAA">Google form</a> (if you do not have a ACCV registration).</center> -->
  </div>
</div>

<center>
<br>
<!-- <a>
    <img width="300px" src="./img/cvpr_banner_homepage.svg" />
</a> -->
<!-- <br> -->
<!-- <a> -->
    <img src="./img/ACCV-Hanoi-Logo_Dk-Blue-1024x226.jpg" style="height:80px" />
    <!-- <img src="./img/teaser.svg"> -->
<!-- </a> -->
</center>

<!-- <br>
<p style="color:red;text-align:center">
<b> &#x1F4E2; &#x1F3A5; Check <a href="https://drive.google.com/">this Google drive link</a> for our tutorial slides and this <a href="https://www.youtube.com/">YouTube link</a> for the video recording.</b>
</p> -->
<br>

<br id="syllabus"/>
<h2><b style='color:brown;'>Overview</b></h2>
<br/>
    <p>
    The goal of this workshop is to showcase the latest developments of generative AI for creating, editing, restoring, and compressing rich media, such as images, videos, neural radiance fields, and 3D scene properties. Generative AI models, such as GAN and diffusion models have enabled remarkable achievements in rich media from both academia research and industrial applications. For instance, cloud-based video gaming is a booming industry with an expected global market value of over $12 billion by 2025. Generative AI transforms the gaming industry by enabling anyone to build and design games without professional artistic and technical skills, empowering immeasurable market growth.
  </p>

  <p>
    With the success of the 1st RichMediaGAI Workshop@WACV 2024, we expand the 2nd RichMediaGAI workshop@ACCV2024 by organizing competitions with industry-level data, soliciting paper submissions, and continuing to invite top-tier speakers from both industry and academia to fuse the synergy.
  </p>
  


  <!--
  <h2><b style='color:red;'>Updates</b></h2>
  <p><b style='color:red;'>Training set available!</b></p>
  <p><b style='color:red;'>Paper submission site open!</b></p>
  <p><b style='color:red;'>Test set available!</b></p>
  -->

  <br id="important_dates"/>
  <h2><b style='color:brown;'>Important Dates + Author Guidelines</b></h2>
  <br/>
      <p>
       <b>Author Guidelines:</b> Formatting, Page Limits, Author Kits, and Submission Policies follow the <b><a href="https://accv2024.org/author-guidelines/">ACCV 2024 Author Guidelines</a></b>     
    </p>
    <p>
      <table style="width:100%"> 
        <tr>
          <td><b>Challenges Data Available at:</b></td>
          <td align="right"><b style='color: brown;'>August 10, 2024, 11:59 PM PST</b></td>
        </tr>
        <tr>
          <td><b>Regular Paper Submission Deadline:</b></td>  
          <td align="right"><b style='color: brown;'>September 23, 2024, 11:59 PM PST</b></td> 
        </tr>
        <tr>
          <td><b>Challenges Results and Reports Submission Deadline:</b></td>
          <td align="right"><b style='color: brown;'>September 23, 2024, 11:59 PM PST</b></td>  
        </tr>
        <tr>
          <td><b>Submission Site:</b></td>
          <td align="right"><b style='color: brown;'>TBD</b></td>         
        </tr>
        <tr>
          <td><b>Paper Review Back and Decision Notification:</b></td>
          <td align="right"><b style='color: brown;'>October 4, 2024, 11:59 PM PST</b></td>        
        </tr>
        <tr>
          <td><b>Challenges Results and Decision Notification:</b></td> 
          <td align="right"><b style='color: brown;'>October 4, 2024, 11:59 PM PST</b></td>        
        </tr>
        <tr>
          <td><b>Camera-Ready Deadline:</b></td> 
          <td align="right"><b style='color: brown;'>October 10, 2024, 11:59 PM PST</b></td>
        </tr>
      </table>
    </p>


  <br id="papers"/>
  <h2><b style='color:brown;'>1. Regular Paper Submissions</b></h2>
  <br/>
  <p>
    Papers addressing topics related to image/video restoration, compression, enhancement, and manipulation, using generative AI technologies are welcome to submit. The topics include but are not limited to: 
            <ul>
              <li> Restoration and enhancement of rich media with generative AI</li>
              <li> Editing and manipulation of rich media with generative AI</li>
              <li> Compression and codec design with generative AI</li>
              <li> Modeling and rendering rich media with generative AI</li>
              <li> Neural radiance fields with generative AI</li>
              <li> Rich media creation and editing with large language models</li>
              <li> Acceleration of generative AI models on edge devices</li>              
              </li>
            </ul>
  </p>
  <p><b>Author Guidelines:</b> Formatting, Page Limits, Author Kits, and Submission Policies follow the <b><a href="https://accv2024.org/author-guidelines/">ACCV 2024 Author Guidelines</a></b>
  All papers must be uploaded to the submission site by the deadline. There is no rebuttal for this call. Reviews and paper decisions will be sent back to the authors on the date specified above. 
   
  
<br id="challenges"/>
<h2><b style='color:brown;'>2. Challenges</b></h2>
<br/>
<p>
  <b><a href=./file/GameIR_CallforChallenge.pdf> Call for Participation</a> </b>
</p>
    <p>
      Cloud gaming poses tremendous challenges for compression and transmission. To avoid delay and bandwidth overload, high-quality frames need to be heavily compressed with very low latency.
      Traditional codecs like H.264/H.265/H.266 or recent neural video coding targeting natural videos generally do not perform well.      
  </p>

  <p>
      Generative AI technologies, e.g., super-resolution, image synthesis and rendering, can largely alleviate the transmission issues. Server-side computation and transmission can be reduced by leveraging the computation power of client de- vices. For example, the server can render low-resolution (LR) frames to transmit, and high-resolution (HR) frames can be computed on client side. In multiview gaming, the server can render part of views to transmit, and the remaining views can be computed by client devices. Nvidia's Deep Learning Super Sampling (DLSS) has commercialized this idea, and one key factor of its success is the large-scale ground-truth LR-HR or multiview gaming data used for training.    
  </p>
  <p>
    In comparison, the research community uses pseudo training data for many restoration tasks. For example, for super-resolution, the LR data is generated from the HR data by downsampling and adding degradation like noises and blurs. Such pseudo data do not match real gaming data. True LR gaming frames are high-quality, sharp and clear without noises or blurs. There are unnatural visual effects and object movements, but with limited motion blur, different from captured natural videos. We need ground-truth gaming data for effective training.
  </p>  
  <p>
    In this competition, a large computer-synthesized ground-truth dataset is provided, targeting two different applications:
            <ul>
              <li> <b>Track 1: Super Resolution in Deferred Rendering</b></li>
              <p>This track aims to restore HR images from LR images along with additional GBuffers during the deferred rendering stage (i.e., segmentation map, depth map), supporting the gaming solution of transferring LR images with assistive information using reduced bits and restoring HR images on client side. 
              </p>
              <p>
                The dataset has 320 LR-HR paired sequences at 1440p and 720p. Each sequence has 60 frames, totalling 19200 LR-HR paired frames. 
                The sequences are rendered by the open source CARLA simulator with the Unreal Engine.
                The paired sequences capture 3D scenes from 8 different towns (20 static scenes and 20 dynamic scenes for each town).         
                The corresponding paired segmentation maps and depth maps synchronize with the RGB images are also provided.     
                Data collected from 7 towns form the training set, and data from the remaining 1 town form the test set. 
              </p>
              <p>
                Given the LR RGB images (720p) and additional GBuffers as input, the task is to develop algorithms to recover the HR RGB images (1440p).                 
              </p>
              <p>Algorithms will be evaluated based on 4 objective quality metrics: PSNR and SSIM to measure pixel-level distortion, LPIPS and FID to measure perceptual quality. 
                The amount of additional GBuffers used will also be considered (the more GBuffers used, the more bits consumed). For example, if two algorithms have similar distortion and quality metrics, the less bits consumed the better.</p>
                <p><a href="https://huggingface.co/datasets/LLLebin/GAMEIR"><b style='color:brown;'>huggingface Download</b></a> includes the following:</p>
                <ul>
                  <li>
                    <b>Mini training set</b>: For each paired video sequence, 5 paired frames are randomly selected to form the mini dataset.
                  </li>
                  <li><b>Full training set</b></li>
                  <li>
                    <b>Code scripts</b> to read data and compute evaluation metrics</b>
                  </li>
                </ul>
                <p>
                <p><b>Challenge result submission:</b> participants must upload the recovered HR RGB images for the test set through (link TBD) </p>

              <li> <b>Track 2: Multiview Video Frame Synthesis</b></li>          
              <p>This track aims to synthesize intermediate frames from a sparse set of input frames in multiview videos, along with camera intrinsic and extrinsic parameters and additional segmentation maps and depth maps, supporting the multiview gaming solution of transferring part of multiview frames with assistive information using reduced bits and generating the remaining frames on client side. 
              </p>
              <p>
                The dataset contains 160 sets of sequences rendered by CARLA simulator. Each set of sequences consists of static 3D scenes captured by six cameras mounted on the top of a car moving within one of the 8 towns. Each town has 20 sets of sequences and each sequence has 60 frames, totaling 57600 frames. 
                The corresponding segmentation maps and depth maps synchronize with the RGB images are also provided.     
              </p>                  
              <p>
                For each set of sequences, a subset of multiview frames will be randomly selected as inputs, and the task is to synthesis the remaining frames. 
              </p>
              <p>
                Algorithms will be evaluated based on 4 objective metrics: PSNR and SSIM to measure pixel-level distortion, LPIPS and FID to measure perceptual quality. The amount of additional GBuffers used will also be considered (the more GBuffers used, the more bits consumed potentially). For example, if two algorithms have similar distortion and quality metrics, the less bits consumed the better.
              </p>
              <p><a href="https://huggingface.co/datasets/LLLebin/GAMEIR"><b style='color:brown;'>huggingface Download</b></a> includes the following:</p>
              <ul>
                <li>
                  <b>Mini training set</b>: For each paired video sequence, 5 paired frames are randomly selected to form the mini dataset.
                </li>
                <li><b>Full training set</b></li>
                <li>
                  <b>Code scripts</b> to read data and compute evaluation metrics</b>
                </li>
              </ul>

              <p><b>Challenge result submission:</b> participants must upload the recovered HR RGB images for the test set through (link TBD) </p>
              </p>
            </ul>
  </p>

  <p>
    The winners will be announced at the RichMediaGAI workshop, and the top 3 non-corporate winners of each track will be rewarded a prize. 
    The winners are invited to submit a paper to the RichMediaGAI workshop through the paper submission system. 
    For the paper to be accepted, each paper must be a self-contained description of the method, and be detailed enough to reproduce the results.
    The paper submission must follow the <b><a href="https://accv2024.org/author-guidelines/">ACCV 2024 Author Guidelines</a></b> . 
  </p>

  <br id="speakers" />
  <br>
  <br>
  <br>
  
  
  <div class="row">
    <div class="col-xs-12">
      <h2><b style='color:brown;'>3. Invited Talks</b></h2>
    </div>
  </div>  
  
  
  <div class="row speaker" id="cl">
    <div class="col-sm-3 speaker-pic">
      <a href="https://www.mmlab-ntu.com/person/ccloy/">
        <img class="people-pic" src="./img/people/ccloy.jpg" />
      </a>
      <div class="people-name">
        <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>
        <h6>Nanyang Technological University</h6>
      </div>
    </div>
    <div class="col-md-9">
      <p class="speaker-bio">
        Chen Change Loy is a President's Chair Professor with the College of Computing and Data Science, Nanyang Technological University, Singapore. He is the Lab Director of MMLab@NTU and Co-associate Director of S-Lab. 
        Prior to joining NTU, he served as a Research Assistant Professor at the MMLab of The Chinese University of Hong Kong, from 2013 to 2018.
        His research interests include computer vision and deep learning with a focus on image/video restoration and enhancement, generative tasks, and representation learning. 
        He serves as an Associate Editor of the International Journal of Computer Vision (IJCV), IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), and Computer Vision and Image Understanding (CVIU). 
        He also serves/served as an Area Chair of top conferences such as ICCV, CVPR, ECCV, ICLR and NeurIPS. He will serve as the Program Co-Chair of CVPR 2026. He is a senior member of IEEE.
      </p>
    </div>

    <div class="row speaker" id="dt">
      <div class="col-sm-3 speaker-pic">
        <a href="https://scholar.google.com/citations?user=VsGLu-QAAAAJ&hl=en">
          <img class="people-pic" src="./img/people/dongtian.jpeg" />
        </a>
        <div class="people-name">
          <a href="https://scholar.google.com/citations?user=VsGLu-QAAAAJ&hl=en">Dong Tian</a> 
          <a href="https://twitter.com/chenhsuan_lin"><img src="./img/Twitter_Social_Icon_Rounded_Square_Color.png" /></a>
          <h6>InterDigital</h6>
        </div>
      </div>
      <div class="col-md-9">
        <p class="speaker-bio">
          Dong Tian is a Senior Director with InterDigital, Inc. He has been actively contributing to MPEG industry standards and academic communities for 20+ years. 
          Prior to InterDigital, Inc. He holds 30+ U.S.-granted patents and 50+ recent publications in top-tier journals/transactions and conferences. 
          His research interests include image processing, 3D video, point cloud processing, and deep learning. 
          He serves as the Chair of MPEG-AI, MPEG 3DGH on AI-Based Graphic Coding since 2021, and MSA TC from 2023 to 2025, and
          a General Co-Chair of MMSP'20 and MMSP'21.
        </p>
      </div>
    </div>

  <div class="row speaker" id="yw">
    <div class="col-sm-3 speaker-pic">
      <a href="https://web.northeastern.edu/yanzhiwang/">
        <img class="people-pic" src="./img/people/Yanzhi_Wang_1400.jpg" />
      </a>
      <div class="people-name">
        <a href="https://web.northeastern.edu/yanzhiwang/">Yanzhi Wang</a>
        <h6>Northeastern Univeristy</h6>
      </div>
    </div>
    <div class="col-md-9">
      <p class="speaker-bio">
      Yanzhi Wang is currently an associate professor and faculty fellow at Dept. of ECE at Northeastern University, Boston, MA. 
      His research interests focus on model compression and platform-specific acceleration of deep learning applications. 
      His work has been published broadly in top conference and journal venues 
      (e.g., DAC, ICCAD, ASPLOS, ISCA, MICRO, HPCA, PLDI, ICS, PACT, ISSCC, AAAI, ICML, NeurIPS, CVPR, ICLR, IJCAI, 
      ECCV, ICDM, ACM MM, FPGA, LCTES, CCS, VLDB, PACT, ICDCS, RTAS, Infocom, C-ACM, JSSC, TComputer, TCAS-I, TCAD, TCAS-I, JSAC, 
      TNNLS, etc.). He has received six Best Paper and Top Paper Awards, 
      and one Communications of the ACM cover featured article. He has another 13 Best Paper Nominations and four Popular Paper Awards. 
      He has received the U.S. Army Young Investigator Program Award (YIP), IEEE TC-SDM Early Career Award, APSIPA Distinguished 
      Leader Award, Massachusetts Acorn Innovation Award, Martin Essigmann Excellence in Teaching Award, Massachusetts Acorn Innovation Award,
       Ming Hsieh Scholar Award, and other research awards from Google, MathWorks, etc. 
      </p>
    </div>
  </div>  
  
  <div class="row speaker" id="tx">
    <div class="col-sm-3 speaker-pic">
      <a href="https://tianfan.info/">
        <img class="people-pic" src="./img/people/xue_tianfan.jpg" />
      </a>
      <div class="people-name">
        <a href="https://tianfan.info/">Tianfan Xue</a>
        <h6>CUHK</h6>
      </div>
    </div>
    <div class="col-md-9">
      <p class="speaker-bio">
      Tianfan Xue is a Vice Chancellor assistant professor at the Department of Information Engineering, 
      the Chinese University of Hong Kong. His research interests include computer vision, machine learning, and computer graphics, 
      with a focus on generative AI and neural rendering.
      </p>
    </div>
  </div>
  

 <!-- 
  <div class="row speaker" id="jy">
    <div class="col-sm-3 speaker-pic">
      <a href="https://scholar.google.com/citations?user=xrND8B8AAAAJ&hl=en">
        <img class="people-pic" src="./img/people/junfeng.jpg" />
      </a>
      <div class="people-name">
        <a href="https://scholar.google.com/citations?user=xrND8B8AAAAJ&hl=en">Junfeng He</a> 
        <a href="https://twitter.com/junyanz89"><img src="./img/Twitter_Social_Icon_Rounded_Square_Color.png" /></a>
        <h6>Google Research</h6>
      </div>
    </div>
    <div class="col-md-9">
      <p class="speaker-bio">
      Junfeng He is with Google Inc. He has published more than 25 papers in top tier conference/journals such as CVPR, ICML, TPAMI, IEEE proceedings, etc., 
      cited by more than 1000 times. He has served as TPC member of ACM MM, CVPR, and several other conferences.
      </p>
    </div>
  </div>

-->

  
<br id="talks" />
<br>
<br>

<div class="row">
  <div class="col-xs-12">
    <h2><b style='color:brown;'>4. Program Schedule (TBD)</b></h2>
    <!--      <p><font color="#76b900"><b>Hover over the titles to view the abstracts of each talk.</b></font></p>
     <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
        <th>Title</th>
        <th>Speaker</th>
        <th>Slides</th>
        <th>Video</th>
        <th>Time&nbsp;&nbsp;&nbsp;</th>
        </tr>
      </thead>
      <tbody>

        <tr>
            <td>Opening Remarks</td>
            <td><a href=""></a>Wei Jiang</td>
            <td><a href="https://drive.google.com/file/d/1NrwM9WpJmXpc2pDw-roS4RDlf4yKTmcj/view?usp=sharing">Link</a></td>
            <td> </td>
            <td><del>09:00 - 09:05</del></td>
        </tr>

        <tr>
            <td>
            <div class="hover-text">
                Data Ownership in Generative Models
                <div class="hidden-info">
                  <p>Large-scale generative visual models, such as DALL·E2 and Stable Diffusion, have made content creation as little effort as writing a short text description. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. How can we remove these images if creators decide to opt-out? How can we properly compensate them if they choose to opt in? </p>

                  <p>In this talk, I will first describe an efficient method for removing copyrighted materials, artistic styles of living artists, and memorized images from pretrained text-to-image diffusion models. I will then discuss our data attribution algorithm for assessing the influence of each training image for a generated sample. Collectively, we aim to enable creators to retain control over the ownership of training images.</p>
                </div>
            </div>
            </td>
            <td><a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></td>
            <td><a href="https://drive.google.com/file/d/1lTfwd2iUwgFqyW-iqWBs2Gl4ajLjtWPK/view?usp=sharing">Link</a></td>
            <td>-- </td>
            <td><del>09:05 - 09:40</del></td>
        </tr>

        <tr>
            <td>
            <div class="hover-text">
            Diffusion Models for 3D Asset Generation
          <div class="hidden-info">
                  <p>3D digital content has been in high demand for a variety of applications, including gaming, entertainment, architecture, and robotics simulation. However, creating such 3D content requires professional 3D modeling expertise with a significant amount of time and effort. In this talk, I will talk about recent advances on automating high-quality 3D digital content creation from text prompts. I will also cover Magic3D, which can generate high-resolution 3D mesh models from input text descriptions, as well as our recent efforts on 3D generative AI with NVIDIA Picasso. With these text-to-3D approaches, we aim to democratize and turbocharge 3D content creation for all, from novices to expert 3D artists.</p>
                </div>
                </div>
            </td>
            <td><a href="https://chenhsuanlin.bitbucket.io/">Chen-Hsuan Lin</a></td>
            <td></td>
            <td> </td>
            <td><del>09:40 - 10:15</del></td>
        </tr>
        <tr>
          <td>Coffee Break</td>
          <td>  </td>
          <td></td>
          <td> </td>
          <td><del>10:15 - 10:25</del></td>
        </tr>

        <tr>
            <td>
              <div class="hover-text">
                  Harnessing Deep Generative Models for Point-Dragging Manipulation and Image Morphing
                <div class="hidden-info">
                  <p>In this talk, I will first present DragGAN, an interactive point-dragging image manipulation method. Unlike previous works that gain controllability of GANs via manually annotated training data or a prior 3D model, DragGAN allows users to "drag" any points of the image to precisely reach target points, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. While DragGAN produces continuous animations, such effects are hard to obtain on diffusion models. So I will then introduce how we address this limitation with DiffMorpher, a method enabling smooth and natural image morphing based on diffusion models.</p>
                </div>
            </div>
            </td>
            <td><a href="https://xingangpan.github.io/">Xingang Pan</a></td>
            <td><a href="https://drive.google.com/file/d/1KBbtMVGuOtkQILLIzA5uEDcKYwtzQU0G/view?usp=sharing">Link</a></td>
            <td> </td>
            <td><del>10:25 - 11:00</del></td>
        </tr>

        <tr>
            <td>
              <div class="hover-text">
                  GPT and Stable Diffusion on the Mobile: Towards Ultimate Efficiency in Deep Learning Acceleration
                <div class="hidden-info">
                  <p>Mobile and embedded computing devices have become key carriers of deep learning to facilitate the widespread use of machine intelligence. However, there is a widely recognized challenge to achieve real-time DNN inference on edge devices, due to the limited computation/storage resources on such devices. Model compression of DNNs, including weight pruning and weight quantization, has been investigated to overcome this challenge. However, current work on DNN compression suffers from the limitation that accuracy and hardware performance are somewhat conflicting goals difficult to satisfy simultaneously.</p>

                  <p>
                  We present our recent work Compression-Compilation Codesign, to overcome this limitation towards the best possible DNN acceleration on edge devices. The neural network model is optimized in a hand-in-hand manner with compiler-level code generation, achieving the best possible hardware performance while maintaining zero accuracy loss, which is beyond the capability of prior work. We are able to achieve real-time on-device execution of a number of DNN tasks, including object detection, pose estimation, activity detection, speech recognition, just using an off-the-shelf mobile device, with up to 180 X speedup compared with prior work. Recently, for the first time, we enable large-scale language and AIGC models such as GPT and Stable Diffusion on mobile devices. We will also introduce our breakthrough in digital avatar, stable diffusion for video generation, and interaction systems. Last we will introduce our recent breakthrough of superconducting logic based neural network acceleration that achieves 10^6 times energy efficiency gain compared with state-of-the-art solutions, achieving the quantum limit in computing.</p>
                </div>
            </div>
            </td>
            <td><a href="https://web.northeastern.edu/yanzhiwang/">Yanzhi Wang</a></td>
            <td><a href=></a></td>
            <td> </td>
            <td><del>11:00 - 11:35</del></td>
        </tr>
      -->
<!--         <tr>
            <td>Panel Discussion</td>
            <td><a href="TODO">ALL</a></td>
            <td>TBA</td>
        </tr>

        <tr>
            <td>Conclusions, Open Problems and Final Remarks</td>
            <td><a href="TODO">ALL</a></td>
            <td>TBA</td>
        </tr> -->


      </tbody>
    </table>
  </div>
</div>

<br id="organizers" />
<br>
<br>
<br>

<div class="row">
  <div class="col-xs-12">
    <h2><b style='color:brown;'>5. Organizers</b></h2>
    <br>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">


  <div class="row">
    <div class="col-xs-4" id="wj">
      <center>
      <a href="https://scholar.google.com/citations?user=c8koDJgAAAAJ">
        <img class="people-pic" src="./img/people/wei_jiang.jpg" />
      </a>
      <div class="people-name">
        <a href="https://scholar.google.com/citations?user=c8koDJgAAAAJ">Wei Jiang</a>
        <h6>Futurewei Technologies</h6>
      </div>
    </center>
  </div>    

    <div class="col-xs-4" id="zx">
      <center>
      <a href="https://scholar.google.com/citations?user=XxKgdDQAAAAJ&hl=en">
        <img class="people-pic" src="./img/people/lebinzhou.png" />
      </a>
      <div class="people-name">
         <a href="https://scholar.google.com/citations?user=XxKgdDQAAAAJ&hl=en">Lebin Zhou</a>
        <h6>Santa Clara University</h6>
      </div>
    </center>
    </div>

    <div class="col-xs-4" id="jw">
      <center>
      <a href="https://www.gujinwei.org">
        <img class="people-pic" src="./img/people/jwgu5.jpg" />
      </a>
      <div class="people-name">
        <a href="https://www.gujinwei.org">Jinwei Gu</a>
        <h6>Chinese Univeristy of Hong Kong</h6>
      </div>
      </center>
    </div>
  </div>
<br/>

    <div class="row">
      <div class="col-xs-4" id="yt">
        <center>
        <a href="https://scholar.google.com/citations?user=GdIW8DUAAAAJ&hl=en">
          <img class="people-pic" src="./img/people/kunhan.jpeg" />
        </a>
        <div class="people-name">
          <a href="https://scholar.google.com/citations?user=GdIW8DUAAAAJ&hl=en">Kun Han</a>
          <h6>University of California Irvine</h6>
        </div>
      </center>
      </div>
      <div class="col-xs-4" id="tf">
      <center>
      <a href="https://scholar.google.com/citations?user=Xk7PH88AAAAJ&hl=en">
        <img class="people-pic" src="./img/people/zijiangyang.jpeg" />
      </a>
      <div class="people-name">
        <a href="https://scholar.google.com/citations?user=Xk7PH88AAAAJ&hl=en">Zijiang (James) Yang</a>
        <h6>Guard Strike</h6>
      </div>
    </center>
    </div>
  </div>


<br />

</div>
<p><b>Contacts</b></p>
<p>Dataset related questions: Lebin Zhou</p>
<p>Paper related and other general questions: <a href="mailto:wjiang@futurewei.com">Wei Jiang</a>, Jinwei Gu</p>
</div>

      </div>
    </div>


<br>
<br>
<br>

<footer class="fixed-footer">
        <div class="container">
            © 2024 Wei Jiang. Template by <a href="https://cvpr2022-tutorial-diffusion-models.github.io/" class="external">Arash Vahdat</a> &#x2192; <a href="https://nvlabs.github.io/eccv2020-limited-labels-data-tutorial/" class="external">Shalini De Mello</a> &#x2192; <a href="https://visualdialog.org/" class="external">https://visualdialog.org/</a>.
        </div>
    </footer>




<!-- 
    <script type="text/javascript" src="/static/js/jquery.min.js"></script>
    <script type="text/javascript" src="/static/js/bootstrap.min.js"></script> -->
     <!-- Bootstrap JS, Popper.js, and jQuery -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!--     <script>
    function showModal() {
        document.getElementById("infoModal").style.display = "block";
    }

    function hideModal() {
        document.getElementById("infoModal").style.display = "none";
    }
  </script> -->

  </body>

</html>
