
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="wacv, workshop, wacv-2024, wacv2024, computer vision, machine learning, generative model, diffusion, LLM">

  <link rel="shortcut icon" href="./img/icon.png">



  <title>Rich Media with Generative AI</title>
  <meta name="description" content="Tutorial in Conjunction with WACV 2024 ---">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="Workshop on Rich Media with Generative AI"/>
  <meta property="og:url" content=""/>
  <meta property="og:description" content="Tutorial in Conjunction with WACV 2024"/>
  <meta property="og:site_name" content="Workshop on Rich Media with Generative AI"/>
  <meta property="og:image" content=""/>
  <meta property="og:image:url" content=""/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:creator" content="">
  <meta name="twitter:title" content=""/>
  <meta name="twitter:image" content="">
  <meta name="twitter:url" content=""/>           
  <meta name="twitter:description" content=""/>

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="./css/bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="./css/main.css" media="screen,projection">

  <!-- Font Awesome -->
  <script src="https://kit.fontawesome.com/ff6e9b10da.js" crossorigin="anonymous"></script>
</head>

  <body>


    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">

    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#syllabus">Syllabus</a></li>
		<li><a href="#speakers">Speakers</a></li>
    <li><a href="#organizers">Orgnizers</a></li>
        <li><a href="#talks">Schedule</a></li>
      </ul>
    </div>

  </div>
</div>

    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
  <center><h2>WACV 2024 Workshop on</h2></center>
    <center><h1>Rich Media with Generative AI</h1></center>
        <center><b>Date: </b>Monday 8 Jan 9:00 am - 11:35 am (Hawaii Time/GMT-10), 2024</center>
        <center><b>Location: </b>Naupaka II + Zoom</center>
        <center>To obtain the Zoom link, kindly fill out this <a href="https://forms.gle/4m8E8vkmTJ4D8vQAA">Google form</a> (if you do not have a WACV registration).</center>
  </div>
</div>

<center>
<br>
<!-- <a>
    <img width="300px" src="./img/cvpr_banner_homepage.svg" />
</a> -->
<!-- <br> -->
<!-- <a> -->
    <img src="./img/WACV-Logo_2024-1024x243.png" style="height:80px" />
    <!-- <img src="./img/teaser.svg"> -->
<!-- </a> -->
</center>

<!-- <br>
<p style="color:red;text-align:center">
<b> &#x1F4E2; &#x1F3A5; Check <a href="https://drive.google.com/">this Google drive link</a> for our tutorial slides and this <a href="https://www.youtube.com/">YouTube link</a> for the video recording.</b>
</p> -->
<br>

<br id="syllabus"/>
<h2>Overview</h2>
<br/>
    <p>
    The goal of this workshop is to showcase the latest developments of generative AI for creating, editing, restoring, and compressing rich media, such as images, videos, audio, neural radiance fields, and 3D scene properties. Rich media are widely used in various domains, such as video games, smartphone photography, visual simulation, AR/VR, medical imaging, and robotics. Generative AI models, such as generative adversial networks (GAN), diffusion models, and large language models (LLM), have enabled remarkable achievements in rich media from both academia research and industrial applications, such as Stable Diffusion, Midjourney, Dream Booth (Google), Dream Booth 3D (Google), Firefly (Adobe), Picasso (NVIDIA), VoiceBox (Meta), DALL-E2 (OpenAI).
  </p>

  <p>
    This workshop aims to provide a platform for researchers in this field to share their latest work and exchange their ideas. The topics in this workshop include but are not limited to:
            <ul>
              <li> Restoration and enhancement of rich media with generative AI</li>
              <li> Editing and manipulation of rich media with generative AI</li>
              <li> Compression and codec design with generative AI</li>
              <li> Image-based modeling and rendering with generative AI</li>
              <li> Neural radiance fields with generative AI</li>
              <li> Rich media creation and editing with large language models</li>
              <li> Acceleration of generative AI models on edge devices</li>
              <li> Bias and Forgery detection in the era of generative AI</li>
              </li>
            </ul>
  </p>



<!-- <div class="row">
  <div class="col-xs-12">
    <h2>Syllabus</h2><br id="syllabus">
  </div>
</div>
 -->


<br id="speakers" />
<br>
<br>
<br>




<div class="row">
  <div class="col-xs-12">
    <h2>Speaks</h2>
  </div>
</div>

<div class="row speaker" id="jy">
  <div class="col-sm-3 speaker-pic">
    <a href="https://www.cs.cmu.edu/~junyanz/">
      <img class="people-pic" src="./img/people/junyan.jpg" />
    </a>
    <div class="people-name">
      <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a> 
      <a href="https://twitter.com/junyanz89"><img src="./img/Twitter_Social_Icon_Rounded_Square_Color.png" /></a>
      <h6>Carnegie Mellon Univeristy</h6>
    </div>
  </div>
  <div class="col-md-9">
    <p class="speaker-bio">
    Jun-Yan Zhu is an Assistant Professor at CMU’s School of Computer Science. Prior to joining CMU, he was a Research Scientist at Adobe Research and a postdoc at MIT CSAIL. He obtained his Ph.D. from UC Berkeley and B.E. from Tsinghua University. He studies computer vision, computer graphics, and computational photography. His current research focuses on generative models for visual storytelling. He has received the Packard Fellowship, the NSF CAREER Award, the ACM SIGGRAPH Outstanding Doctoral Dissertation Award, and the UC Berkeley EECS David J. Sakrison Memorial Prize for outstanding doctoral research, among other awards.

    </p>
  </div>
</div>




<div class="row speaker" id="my">
  <div class="col-sm-3 speaker-pic">
    <a href="https://chenhsuanlin.bitbucket.io/">
      <img class="people-pic" src="./img/people/chen-hsuan.png" />
    </a>
    <div class="people-name">
      <a href="https://chenhsuanlin.bitbucket.io/">Chen-Hsuan Lin</a> 
      <a href="https://twitter.com/chenhsuan_lin"><img src="./img/Twitter_Social_Icon_Rounded_Square_Color.png" /></a>
      <h6>NVIDIA</h6>
    </div>
  </div>
  <div class="col-md-9">
    <p class="speaker-bio">
    Chen-Hsuan Lin is a senior research scientist at NVIDIA Research, working on computer vision, computer graphics, and generative AI applications. He is interested in solving problems for 3D content creation, involving 3D reconstruction, neural rendering, generative models, and beyond. His research aims to empower AI systems with 3D visual intelligence: human-level 3D perception and imagination abilities. His research has been recognized with a Best Inventions of 2023 by TIME Magazine.
    </p>
  </div>
</div>



<div class="row speaker" id="xg">
  <div class="col-sm-3 speaker-pic">
    <a href="https://xingangpan.github.io/">
      <img class="people-pic" src="./img/people/XingangPan.png" />
    </a>
    <div class="people-name">
      <a href="https://xingangpan.github.io/">Xingang Pan</a>
      <a href="https://twitter.com/XingangP"><img src="./img/Twitter_Social_Icon_Rounded_Square_Color.png" /></a>
      <h6>Nanyang Technological Univeristy</h6>
    </div>
  </div>
  <div class="col-md-9">
    <p class="speaker-bio">
    Xingang Pan is an Assistant Professor with the School of Computer Science and Engineering at Nanyang Technological University, affiliated with MMLab-NTU and S-Lab. Prior to joining NTU, he was a postdoc researcher at Max Planck Institute for Informatics, advised by Prof. Christian Theobalt. He received his Ph.D. degree at MMLab of The Chinese University of Hong Kong in 2021, supervised by Prof. Xiaoou Tang. He obtained his Bachelor’s degree from Tsinghua University in 2016. His research interests include computer vision, machine learning, and computer graphics, with a focus on generative AI and neural rendering.
    </p>
  </div>
</div>


<div class="row speaker" id="xg">
  <div class="col-sm-3 speaker-pic">
    <a href="https://web.northeastern.edu/yanzhiwang/">
      <img class="people-pic" src="./img/people/Yanzhi_Wang_1400.jpg" />
    </a>
    <div class="people-name">
      <a href="https://web.northeastern.edu/yanzhiwang/">Yanzhi Wang</a>
       <!-- <a href=""><img src="./img/Twitter_Social_Icon_Rounded_Square_Color.png" /></a> -->
      <h6>Northeastern Univeristy</h6>
    </div>
  </div>
  <div class="col-md-9">
    <p class="speaker-bio">
    Yanzhi Wang is currently an associate professor and faculty fellow at Dept. of ECE at Northeastern University, Boston, MA. He received the B.S. degree from Tsinghua University in 2009, and Ph.D. degree from University of Southern California in 2014. His research interests focus on model compression and platform-specific acceleration of deep learning applications. His work has been published broadly in top conference and journal venues (e.g., DAC, ICCAD, ASPLOS, ISCA, MICRO, HPCA, PLDI, ICS, PACT, ISSCC, AAAI, ICML, NeurIPS, CVPR, ICLR, IJCAI, ECCV, ICDM, ACM MM, FPGA, LCTES, CCS, VLDB, PACT, ICDCS, RTAS, Infocom, C-ACM, JSSC, TComputer, TCAS-I, TCAD, TCAS-I, JSAC, TNNLS, etc.), and has been cited above 16,000 times. He has received six Best Paper and Top Paper Awards, and one Communications of the ACM cover featured article. He has another 13 Best Paper Nominations and four Popular Paper Awards. He has received the U.S. Army Young Investigator Program Award (YIP), IEEE TC-SDM Early Career Award, APSIPA Distinguished Leader Award, Massachusetts Acorn Innovation Award, Martin Essigmann Excellence in Teaching Award, Massachusetts Acorn Innovation Award, Ming Hsieh Scholar Award, and other research awards from Google, MathWorks, etc. He has received 26 federal grants from NSF, DARPA, IARPA, ARO, ARFL/AFOSR, Dept. of Homeland Security, etc. He has participated in a total of $40M funds with personal share $8.5M. 11 of his academic descendants become tenure track faculty at Univ. of Connecticut, Clemson University, Chongqing University, University of Georgia, Beijing University of Technology, University of Texas San Antonio, and Cleveland State University. They have secured around $5M personal share in funds.
    </p>
  </div>
</div>



<br id="talks" />
<br>
<br>

<div class="row">
  <div class="col-xs-12">
     <h2>Schedule</h2>
     <p><font color="#76b900"><b>Hover over the titles to view the abstracts of each talk.</b></font></p>
     <table class="table schedule" style="border:none !important;">
      <thead class="thead-light">
        <tr>
        <th>Title</th>
        <th>Speaker</th>
        <th>Time (GMT-10)</th>
        </tr>
      </thead>
      <tbody>

        <tr>
            <td>Opening Remarks</td>
            <td><a href=""></a></td>
            <td>09:00 - 09:05</td>
        </tr>

        <tr>
            <td>
              <!-- <span onmouseover="showModal()" onmouseout="hideModal()">Data Ownership in Generative Models</span> -->
            <div class="hover-text">
                Data Ownership in Generative Models
                <div class="hidden-info">
                  <p>Large-scale generative visual models, such as DALL·E2 and Stable Diffusion, have made content creation as little effort as writing a short text description. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. How can we remove these images if creators decide to opt-out? How can we properly compensate them if they choose to opt in? </p>

                  <p>In this talk, I will first describe an efficient method for removing copyrighted materials, artistic styles of living artists, and memorized images from pretrained text-to-image diffusion models. I will then discuss our data attribution algorithm for assessing the influence of each training image for a generated sample. Collectively, we aim to enable creators to retain control over the ownership of training images.</p>
        <!-- Add more content as needed -->
                </div>
            </div>
            </td>
            <td><a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></td>
            <td>09:05 - 09:40</td>
        </tr>

        <tr>
            <td>
            <div class="hover-text">
            Diffusion Models for 3D Asset Generation
          <div class="hidden-info">
                  <p>3D digital content has been in high demand for a variety of applications, including gaming, entertainment, architecture, and robotics simulation. However, creating such 3D content requires professional 3D modeling expertise with a significant amount of time and effort. In this talk, I will talk about recent advances on automating high-quality 3D digital content creation from text prompts. I will also cover Magic3D, which can generate high-resolution 3D mesh models from input text descriptions, as well as our recent efforts on 3D generative AI with NVIDIA Picasso. With these text-to-3D approaches, we aim to democratize and turbocharge 3D content creation for all, from novices to expert 3D artists.</p>
                </div>
                </div>
            </td>
            <td><a href="https://chenhsuanlin.bitbucket.io/">Chen-Hsuan Lin</a></td>
            <td>09:40 - 10:15</td>
        </tr>
        <tr>
          <td>Coffee Break</td>
          <td>  </td>
          <td>10:15 - 10:25</td>
        </tr>

        <tr>
            <td>
              <div class="hover-text">
                  Harnessing Deep Generative Models for Point-Dragging Manipulation and Image Morphing
                <div class="hidden-info">
                  <p>In this talk, I will first present DragGAN, an interactive point-dragging image manipulation method. Unlike previous works that gain controllability of GANs via manually annotated training data or a prior 3D model, DragGAN allows users to "drag" any points of the image to precisely reach target points, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. While DragGAN produces continuous animations, such effects are hard to obtain on diffusion models. So I will then introduce how we address this limitation with DiffMorpher, a method enabling smooth and natural image morphing based on diffusion models.</p>
                </div>
            </div>
            </td>
            <td><a href="https://xingangpan.github.io/">Xingang Pan</a></td>
            <td>10:25 - 11:00</td>
        </tr>

        <tr>
            <td>
              <div class="hover-text">
                  GPT and Stable Diffusion on the Mobile: Towards Ultimate Efficiency in Deep Learning Acceleration
                <div class="hidden-info">
                  <p>Mobile and embedded computing devices have become key carriers of deep learning to facilitate the widespread use of machine intelligence. However, there is a widely recognized challenge to achieve real-time DNN inference on edge devices, due to the limited computation/storage resources on such devices. Model compression of DNNs, including weight pruning and weight quantization, has been investigated to overcome this challenge. However, current work on DNN compression suffers from the limitation that accuracy and hardware performance are somewhat conflicting goals difficult to satisfy simultaneously.</p>

                  <p>
                  We present our recent work Compression-Compilation Codesign, to overcome this limitation towards the best possible DNN acceleration on edge devices. The neural network model is optimized in a hand-in-hand manner with compiler-level code generation, achieving the best possible hardware performance while maintaining zero accuracy loss, which is beyond the capability of prior work. We are able to achieve real-time on-device execution of a number of DNN tasks, including object detection, pose estimation, activity detection, speech recognition, just using an off-the-shelf mobile device, with up to 180 X speedup compared with prior work. Recently, for the first time, we enable large-scale language and AIGC models such as GPT and Stable Diffusion on mobile devices. We will also introduce our breakthrough in digital avatar, stable diffusion for video generation, and interaction systems. Last we will introduce our recent breakthrough of superconducting logic based neural network acceleration that achieves 10^6 times energy efficiency gain compared with state-of-the-art solutions, achieving the quantum limit in computing.</p>
                </div>
            </div>
            </td>
            <td><a href="https://web.northeastern.edu/yanzhiwang/">Yanzhi Wang</a></td>
            <td>11:00 - 11:35</td>
        </tr>
<!--         <tr>
            <td>Panel Discussion</td>
            <td><a href="TODO">ALL</a></td>
            <td>TBA</td>
        </tr>

        <tr>
            <td>Conclusions, Open Problems and Final Remarks</td>
            <td><a href="TODO">ALL</a></td>
            <td>TBA</td>
        </tr> -->


      </tbody>
    </table>
  </div>
</div>

<br id="organizers" />
<br>
<br>
<br>

<div class="row">
  <div class="col-xs-12">
    <h2>Organizers</h2><br>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">


  <div class="row">
    <div class="col-xs-4" id="zx">
      <center>
      <a href="https://lightchaserx.github.io/">
        <img class="people-pic" src="./img/people/zhixiang.jpg" />
      </a>
      <div class="people-name">
         <a href="https://lightchaserx.github.io/">Zhixiang Wang</a>
        <h6>Univeristy of Tokyo</h6>
      </div>
    </center>
    </div>
    <div class="col-xs-4" id="yt">
      <center>
      <a href="">
        <img class="people-pic" src="./img/people/yitong.jpg" />
      </a>
      <div class="people-name">
        <a href="">Yitong Jiang</a>
        <h6>Chinese Univeristy of Hong Kong</h6>
      </div>
    </center>
    </div>
    <div class="col-xs-4" id="wj">
      <center>
      <a href="https://scholar.google.com/citations?user=c8koDJgAAAAJ">
        <img class="people-pic" src="./img/people/wei_jiang.jpg" />
      </a>
      <div class="people-name">
        <a href="https://scholar.google.com/citations?user=c8koDJgAAAAJ">Wei Jiang</a>
        <h6>Futurewei</h6>
      </div>
    </center>
  </div>
</div>
<br/>

    <div class="row">
    <div class="col-xs-4" id="tf">
      <center>
      <a href="https://tianfan.info/">
        <img class="people-pic" src="./img/people/xue_tianfan.jpg" />
      </a>
      <div class="people-name">
        <a href="https://tianfan.info/">Tianfan Xue</a>
        <h6>Chinese Univeristy of Hong Kong</h6>
      </div>
    </center>
    </div>
    <div class="col-xs-4" id="jw">
      <center>
      <a href="https://www.gujinwei.org">
        <img class="people-pic" src="./img/people/jwgu5.jpg" />
      </a>
      <div class="people-name">
        <a href="https://www.gujinwei.org">Jinwei Gu</a>
        <h6>Chinese Univeristy of Hong Kong</h6>
      </div>
    </center>
    </div>
  </div>







<br />

</div></div>

      </div>
    </div>


<br>
<br>
<br>

<footer class="fixed-footer">
        <div class="container">
            © 2023 Zhixiang Wang. Template by <a href="https://cvpr2022-tutorial-diffusion-models.github.io/" class="external">Arash Vahdat</a> &#x2192; <a href="https://nvlabs.github.io/eccv2020-limited-labels-data-tutorial/" class="external">Shalini De Mello</a> &#x2192; <a href="https://visualdialog.org/" class="external">https://visualdialog.org/</a>.
        </div>
    </footer>




<!-- 
    <script type="text/javascript" src="/static/js/jquery.min.js"></script>
    <script type="text/javascript" src="/static/js/bootstrap.min.js"></script> -->
     <!-- Bootstrap JS, Popper.js, and jQuery -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

<!--     <script>
    function showModal() {
        document.getElementById("infoModal").style.display = "block";
    }

    function hideModal() {
        document.getElementById("infoModal").style.display = "none";
    }
  </script> -->

  </body>

</html>
